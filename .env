# =========================
# OCI GenAI (Oracle Cloud)
# =========================
# OCI user OCID used for signing requests to Oracle Cloud.
OCI_USER_OCID=ocid1.user.oc1..aaaaaaaa5cq3iewffep5nzqb7qzoe6mpj45gt4kndvzwvuxzzavpbiucqqaq
# Path to your OCI API private key used with the fingerprint below.
OCI_KEY_FILE=/home/ubuntu/.oci/oci_api_key.pem
# Fingerprint of the public key you uploaded to OCI console for the above private key.
OCI_KEY_FINGERPRINT=de:50:15:13:af:bd:76:fa:f4:77:ad:d4:af:70:a5:d6
# Tenancy OCID of your Oracle Cloud account.
OCI_TENANCY_OCID=ocid1.tenancy.oc1..aaaaaaaafhegmvy2da7xzh2b5jbmhdkfr4cr4e37m5filt4zgxs6mfl7icua
# Region where your OCI resources live (e.g., ap-sydney-1, us-chicago-1).
OCI_REGION=us-chicago-1
# Compartment OCID where GenAI resources are located.
OCI_COMPARTMENT_OCID=ocid1.compartment.oc1..aaaaaaaacoqxp2n77ra2343maw2px4rlrtzqaw5ord6be2cbrbwlrpqwegxa
# Specific Generative AI model OCID to use for Oracle GenAI endpoints.
OCI_GENAI_MODEL_OCID=ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceyayjawvuonfkw2ua4bob4rlnnlhs522pafbglivtwlfzta
# If using ~/.oci/config profiles, set the profile name (default: DEFAULT)
# OCI_CONFIG_PROFILE=DEFAULT

# =========================
# PostgreSQL / PGVector
# =========================
# Hostname/IP of your PostgreSQL server.
AUSLEGALSEARCH_DB_HOST=10.150.2.230
# PostgreSQL port (default 5432).
AUSLEGALSEARCH_DB_PORT=5432
# Database user for ingestion and query.
AUSLEGALSEARCH_DB_USER=postgres
# Database user password (keep secret; do not commit .env).
AUSLEGALSEARCH_DB_PASSWORD=RAbbithole1234##
# Database name/schema to use.
AUSLEGALSEARCH_DB_NAME=postgres

# Optional full SQLAlchemy DSN (overrides the above when set). Example:
# AUSLEGALSEARCH_DB_URL='postgresql+psycopg2://user:pass@host:5432/dbname'

# =========================
# Oracle 23ai (optional)
# =========================
# Uncomment and set if connecting to Oracle 23ai.
# ORACLE_DB_USER=your_oracle_db_user
# ORACLE_DB_PASSWORD=your_oracle_db_password
# ORACLE_DB_DSN=your_oracle_db_tns_or_service
# ORACLE_WALLET_LOCATION=/path/to/wallet/dir

# =========================
# Embeddings / Models
# =========================
# Sentence-Transformers or HF model name for embeddings.
AUSLEGALSEARCH_EMBED_MODEL=nomic-ai/nomic-embed-text-v1.5
## If using Offline Model then use below for AUSLEGALSEARCH_EMBED_MODEL
#AUSLEGALSEARCH_EMBED_MODEL="/opt/hf-cache/nomic-embed-text-v1.5"
# Embedding dimensionality for pgvector column (must match model, e.g., 768).
AUSLEGALSEARCH_EMBED_DIM=768
# Optional HF cache directory on fast disk for model weights.
# HF_HOME=/fast/ssd/hf_cache
# (Optional) Disable tokenizer multithreading warnings (correct var name).
# TOKENIZERS_PARALLELISM=false
# Allow custom HF model code (required for some repos like nomic-ai). 1=allow
AUSLEGALSEARCH_TRUST_REMOTE_CODE=1
# Optional: pin a specific HF revision/tag/commit to avoid pulling latest code/assets (stabilizes cold starts)
# Example: "refs/tags/v1.5" or a specific commit hash supported by the repo
# AUSLEGALSEARCH_EMBED_REV=refs/tags/v1.5
# Optional: run strictly from local HF cache (no network). Set to 1 after you prefetch the model.
# AUSLEGALSEARCH_HF_LOCAL_ONLY=1
# If you fully prefetch and want to enforce offline mode across transformers, you can also set:
# TRANSFORMERS_OFFLINE=1
# Maximum sequence length for HF fallback (tokenizer truncation), default 512
# AUSLEGALSEARCH_EMBED_MAXLEN=512
# Internal flags for embedder behavior (space-separated, e.g. "trust_remote_code")
# AUSLEGALSEARCH_EMBEDDER_FLAGS=

# (Optional) HF transfer engine toggle; disable to avoid rust transfer, enable for resumable high-speed
# HF_HUB_ENABLE_HF_TRANSFER=0

# FULL OFFLINE MODE FOR EMBEDDINGS (Git LFS) — HOW TO
# 1) Prepare a persistent cache folder:
#    sudo mkdir -p /opt/hf-cache
#    sudo chown -R $USER:$USER /opt/hf-cache
#    export HF_HOME=/opt/hf-cache
# 2) Install Git LFS and clone the model:
#    sudo apt-get update && sudo apt-get install -y git git-lfs
#    git lfs install
#    cd /opt/hf-cache
#    git clone https://huggingface.co/nomic-ai/nomic-embed-text-v1.5
#    cd nomic-embed-text-v1.5
#    git lfs pull
#    (This fetches model.safetensors and related files; you can optionally remove onnx/ to save space.)
# 3) In .env, set the model path and offline flags when you want fully offline runs:
#    AUSLEGALSEARCH_EMBED_MODEL="/opt/hf-cache/nomic-embed-text-v1.5"
#    TRANSFORMERS_OFFLINE=1
#    AUSLEGALSEARCH_HF_LOCAL_ONLY=1
#    HF_HOME=/opt/hf-cache
# 4) Load the .env into your shell before running:
#    set -a; source .env; set +a
# 5) Validate:
#    python3 -c "from embedding.embedder import Embedder; e=Embedder(); print('model=',e.model_name,'dim=',e.dimension)"

# =========================
# Ingestion / Worker Flags
# =========================
# Auto-run create_all_tables() on import (set 0 when schema is pre-created).
AUSLEGALSEARCH_AUTO_DDL=1
# Per-GPU embedding batch size (worker adapts down on OOM automatically).
AUSLEGALSEARCH_EMBED_BATCH=32
# Light schema init for first runs on new instances to avoid heavy DB backfills and large index builds.
# 0 = full schema init (backfill FTS, build vector index); 1 = skip heavy steps (you can run them later).
AUSLEGALSEARCH_SCHEMA_LIGHT_INIT=1
# Unbuffered Python output so logs flush immediately (recommended for long-running workers/orchestrator).
PYTHONUNBUFFERED=1
# CPU parallelism for parse+chunk stage (Stage A, multiprocessing):
# - Set to number of CPU worker processes per GPU worker.
# - 0 or unset -> auto: min(physical_cores-1, 8), at least 1
# - Set >1 to enable the pipelined mode (CPU pool feeding GPU).
AUSLEGALSEARCH_CPU_WORKERS=8
# Prefetch buffer size (number of files prepared ahead of GPU embedding).
# - Controls how many CPU-prepared files are queued to keep the GPU fed.
# - 0 or unset -> default 64
# - Increase on larger RAM/GPUs (e.g., 128–256) to maximize overlap.
AUSLEGALSEARCH_PIPELINE_PREFETCH=128
# Sort each worker's assigned files by size (descending) to reduce tail latency.
# 1 = enabled (default), 0 = disable (keep natural order).
AUSLEGALSEARCH_SORT_WORKER_FILES=1

# Include per-file metrics & timings (parse_ms, chunk_ms, embed_ms, insert_ms).
AUSLEGALSEARCH_LOG_METRICS=1
# Emit detailed error records to {child}.errors.ndjson (1=enabled).
AUSLEGALSEARCH_ERROR_DETAILS=1
# Include Python tracebacks in error records (0=omit, 1=include).
AUSLEGALSEARCH_ERROR_TRACE=1
# Print periodic DB document/embedding counts (debug only; default 0)
# AUSLEGALSEARCH_DEBUG_COUNTS=0

# =========================
# Chunking / Timeouts
# =========================
# Per-stage deadlines (seconds). Stuck files are aborted and will fallback or error.
AUSLEGALSEARCH_TIMEOUT_PARSE=30
AUSLEGALSEARCH_TIMEOUT_CHUNK=60
AUSLEGALSEARCH_TIMEOUT_EMBED_BATCH=180
# Per-file DB insert deadline (seconds). Retries occur with exponential backoff.
AUSLEGALSEARCH_TIMEOUT_INSERT=120
# Regex timeout for semantic_chunker operations (ms); requires the 'regex' package.
# Lower this if you see stalls during "chunk start". Defaults to 200ms if unset.
AUSLEGALSEARCH_REGEX_TIMEOUT_MS=200
# Enable RCTS fallback for unstructured generic text (requires langchain-text-splitters + tiktoken).
AUSLEGALSEARCH_USE_RCTS_GENERIC=1
# Character-window fallback chunker sizes, used when semantic chunking times out/errors.
AUSLEGALSEARCH_FALLBACK_CHARS_PER_CHUNK=4000
AUSLEGALSEARCH_FALLBACK_OVERLAP_CHARS=200
# Enable character-window fallback when chunking times out/errors (default: 1)
# AUSLEGALSEARCH_FALLBACK_CHUNK_ON_TIMEOUT=1

# =========================
# Database Pooling / Timeouts (Prod)
# =========================
# Size of SQLAlchemy connection pool per process (workers open their own pools).
AUSLEGALSEARCH_DB_POOL_SIZE=10
# Extra connections allowed beyond pool_size during bursts.
AUSLEGALSEARCH_DB_MAX_OVERFLOW=20
# Recycle connections periodically to avoid server idle timeouts (seconds).
AUSLEGALSEARCH_DB_POOL_RECYCLE=1800
# Max seconds to wait for a free pooled connection before raising.
AUSLEGALSEARCH_DB_POOL_TIMEOUT=30
# Server-side statement timeout (milliseconds) applied to new connections (optional).
# AUSLEGALSEARCH_DB_STATEMENT_TIMEOUT_MS=60000
# Client-side per-file DB SELECT deadline (seconds) for session-file row checks (prevents pre-parse stalls).
AUSLEGALSEARCH_TIMEOUT_SELECT=30
# Max retries for transient DB errors on per-file SELECT/INSERT with exponential backoff.
AUSLEGALSEARCH_DB_MAX_RETRIES=5

# =========================
# GPU / Performance Tuning (recommended)
# =========================
# Mitigate CUDA memory fragmentation and allocator churn (helps avoid stalls/OOM thrash).
# See: https://pytorch.org/docs/stable/notes/cuda.html#environment-variables
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
# Limit math library threads per worker to reduce CPU contention on multi-GPU runs.
OMP_NUM_THREADS=1
MKL_NUM_THREADS=1
OPENBLAS_NUM_THREADS=1
NUMEXPR_NUM_THREADS=1
# Reduce glibc heap arena fragmentation under heavy multi-threaded loads.
MALLOC_ARENA_MAX=2
# Pin workers to specific GPU(s) when launching (set per-process)
# CUDA_VISIBLE_DEVICES=0

# =========================
# API / Backend (optional, if using FastAPI/Gradio/Streamlit auth)
# =========================
# Basic auth user/pass for local FastAPI admin endpoints or internal tooling.
# FASTAPI_API_USER=legal_api
# FASTAPI_API_PASS=letmein
# Public base URL for the API/UI if needed by clients.
# AUSLEGALSEARCH_API_URL=http://localhost:8000

# =========================
# Notes
# =========================
# - Do NOT commit real secrets to source control.
# - Use environment managers (systemd, Docker secrets, cloud secret stores) in production.
# - Ensure AUSLEGALSEARCH_EMBED_DIM matches the selected embedding model.
# - For fully offline embeddings: set AUSLEGALSEARCH_EMBED_MODEL to the local path (e.g., /opt/hf-cache/nomic-embed-text-v1.5)
#   and set TRANSFORMERS_OFFLINE=1 and AUSLEGALSEARCH_HF_LOCAL_ONLY=1 after prefetching via Git LFS as above.
